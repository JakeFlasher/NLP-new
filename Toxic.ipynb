{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_code_all_hidden": true,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Toxic.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JakeFlasher/NLP-new/blob/master/Toxic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "9BLcheCzzy3H"
      },
      "source": [
        "# <span style=\"color:#cc6699\">  Toxic Spans Detection </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "ERLRRJrdzy3K"
      },
      "source": [
        "<span style=\"color:#cc6699\"> SemEval competition</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "w3K0nX_Rzy3K"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_AthwZG0Ndk"
      },
      "source": [
        "!pip install transformers\n",
        "!/opt/bin/nvidia-smi\n",
        "!pip install fsspec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "KnUxXEQUzy3L"
      },
      "source": [
        "#library to check if all is run good in for_loop\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from ast import literal_eval\n",
        "\n",
        "import json\n",
        "\n",
        "# Data manipulation/analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Setting random seed\n",
        "seed = 123\n",
        "\n",
        "# Data partitioning\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Text preprocessing/analysis\n",
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning models\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import neighbors\n",
        "from sklearn import linear_model #for logistic regression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras import Sequential, Model, Input\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "#seeds\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "# tensorflow.random.set_random_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFZA6rlazy3O"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "WrR2aOpvzy3O"
      },
      "source": [
        "# Datasets Loading\n",
        "# ind = 2\n",
        "# train_url = 'https://raw.githubusercontent.com/JakeFlasher/NLP-new/master/data/train_' + str(ind) + '.csv'\n",
        "# print(train_url)\n",
        "# test_url = 'https://raw.githubusercontent.com/JakeFlasher/NLP-new/master/data/fold_' + str(ind) + '.csv'\n",
        "trial_url = 'https://raw.githubusercontent.com/ipavlopoulos/toxic_spans/master/data/tsd_trial.csv'\n",
        "train_url = 'https://raw.githubusercontent.com/ipavlopoulos/toxic_spans/master/data/tsd_train.csv'\n",
        "test_url = 'https://raw.githubusercontent.com/ipavlopoulos/toxic_spans/master/data/tsd_test.csv'\n",
        "# training data\n",
        "\n",
        "dataset = pd.read_csv(train_url)\n",
        "# development and validation data\n",
        "tsd = pd.read_csv(trial_url) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hideOutput": false,
        "hidePrompt": false,
        "id": "buPCYlRIzy3P"
      },
      "source": [
        "Example: \"This is a stupid example, so thank you for nothing a!@#!@.\"\n",
        "\n",
        "It comprises two toxic spans, <span style=\"color:Red\"> \"stupid\"</span> and <span style=\"color:Blue\">\"a!@#!@\"</span>, which have character offsets from 10 to 15 (counting starts from 0) and from 51 to 56 respectively. Systems are then expected to return the following list for this text:\n",
        "\n",
        "[<span style=\"color:Red\"> 10,11,12,13,14,15,</span> <span style=\"color:Blue\"> 51,52,53,54,55,56</span>]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3GOv1yPzy3P"
      },
      "source": [
        "# if we load a cell that it has list, it will need to\n",
        "# build the list,otherwise it read as series of characters\n",
        "dataset.spans = dataset.spans.apply(literal_eval)\n",
        "dataset.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKh3HQAfzy3P"
      },
      "source": [
        "# if we load a cell that it has list, it will need to\n",
        "# build the list,otherwise it read as series of characters\n",
        "tsd.spans = tsd.spans.apply(literal_eval)\n",
        "tsd.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBdCqS6Rzy3R"
      },
      "source": [
        "# Exploratory text analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoKD_MIKzy3R"
      },
      "source": [
        "# check if we have null data to remove-dropna\n",
        "dataset.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-9DvJZYzy3R"
      },
      "source": [
        "# extract the the toxic word from training set\n",
        "\n",
        "all_bad_word = []\n",
        "for text in range(0,len(dataset['text'])):\n",
        "    count = 0 #count how many potition are continuous\n",
        "    \n",
        "    bad_word = []\n",
        "    for i in range(0, len(dataset.spans[text])):\n",
        "        bad_word.append(dataset.text[text][dataset.spans[text][i]])\n",
        "    all_bad_word.append(''.join(bad_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgxuq-Rqzy3S"
      },
      "source": [
        "all_bad_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-b4hLWJzy3S"
      },
      "source": [
        "We can observe that we have concat word without space, for instanse, 'honkeyidiotsdimwitted' = 'honky idiots dimwitted'. Futhermore, we have f**k = fuck and sentences with bad meaning only if we can see as sentence 'three black men in America is a felon'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8je62cVzy3S"
      },
      "source": [
        "# find all Punctuation in words  \n",
        "import string \n",
        "\n",
        "for sentence in all_bad_word: \n",
        "    for i in sentence: \n",
        "        # checking whether the char is punctuation. \n",
        "        if i in string.punctuation: \n",
        "          \n",
        "            # Printing the punctuation values  \n",
        "            print(\"Punctuation: \" + sentence) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnlghrfNzy3T"
      },
      "source": [
        "We can see that punctuation like `*`, `---repeat`, `/`, `$` and `'someting'` play key role, the others are in sentence:\n",
        "\n",
        "For instance:\n",
        "`s%#cks`-> keep secret the toxic word\n",
        "\n",
        "`foola$$ `-> keep secret the toxic word\n",
        "\n",
        "`fu/kis`-> keep secret the toxic word\n",
        "\n",
        "`idiotF-----g` -> keep secret the toxic word\n",
        "\n",
        "`f*** you` -> keep secret the toxic word\n",
        "\n",
        "`'cool'` -> reveal ironic\n",
        "\n",
        "We observe the repeat punctuation hide toxic words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY35cAKazy3T"
      },
      "source": [
        "Moreover, we can see that hastags hide text(eg `#yourwhitetearsarentsacredthislandis`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDaSCloHzy3U"
      },
      "source": [
        "count_non_toxic = 0\n",
        "for element in all_bad_word:\n",
        "    if element == '' :\n",
        "        count_non_toxic +=1\n",
        "        \n",
        "print('How many text not have toxic words: ' + str(count_non_toxic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7YYf6Jbzy3U"
      },
      "source": [
        "from https://towardsdatascience.com/exploratory-text-analysis-in-python-8cf42b758d9e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "908Hb160zy3U"
      },
      "source": [
        "# Prepare training corpus into one giant string\n",
        "train_string = \" \".join(dataset.text)\n",
        "print(f\"***** Extract of a train string ***** \\n{train_string[:200]}\", \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkm78_Kjzy3V"
      },
      "source": [
        "# Split train_corpus by white space\n",
        "splits = train_string.split()  \n",
        "print(f\"***** Extract of splits ***** \\n{splits[:20]}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zSx3Pbzzy3V"
      },
      "source": [
        "print(f\"Number of strings: {len(splits)}\")\n",
        "print(f\"Number of unique strings: {len(set(splits))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZJGfgryzy3V"
      },
      "source": [
        "There are approximately 300.000 strings in the training corpus with around 40.000 unique strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ayFZKpzy3V"
      },
      "source": [
        "#frequency distribution for each string\n",
        "freq_splits = FreqDist(splits)\n",
        "print(f\"***** 10 most common strings ***** \\n{freq_splits.most_common(10)}\", \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH_NQG_Vzy3W"
      },
      "source": [
        "It is obvious that the most common strings are stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOagjlGHzy3W"
      },
      "source": [
        "# we find all the words that they have less than 4 characters (small words)\n",
        "short = set(s for s in splits if len(s)<4)\n",
        "short = [(s, freq_splits[s]) for s in short]\n",
        "short.sort(key=lambda x:x[1], reverse=True)\n",
        "short"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KpsF0Qxzy3W"
      },
      "source": [
        "Many short strings appear to be stop words (like `or`) but there are also numbers(like `2`) and other short words(like `ass`).\n",
        "\n",
        "Because we haven’t tokenised yet, some strings currently contain punctuation that is attached to a word (like `it.` or `\"I`). As a result, otherwise same words are considered as different like `it`,`It`, `it?`, `it.`. Τhey probably need to normalize.\n",
        "\n",
        "There are numbers in different forms like `('70s', 2)`,`('97%', 2)`,`('1.6', 2)` and `('$2', 2),`. Τhey probably do not need in our case, because a number is not toxic word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CAn9Sfazy3W"
      },
      "source": [
        "# we find all the words that they have more than 15 characters (longest words)\n",
        "long = set(s for s in splits if len(s)>15)\n",
        "long = [(s, freq_splits[s]) for s in long]\n",
        "long.sort(key=lambda x:x[1], reverse=True)\n",
        "long"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5R48u9Bzy3W"
      },
      "source": [
        "The frequency of long string looks much lower than short strings. \n",
        "\n",
        "For instance `('the', 11379)` vs `('responsibilities', 3)`\n",
        "\n",
        "There are hyphenated words: `Socio-Communists`, `we-are-shareholders`, `non-contributors` and `discrimination--except`(this one with double hyphens). If we tokenise on white space or punctuation, these strings will be split into separate words. For most cases, this will conserve the gist of the sentence. If we keep hyphenated words as they are, they won’t be as common and consequently removed as rare words.\n",
        "\n",
        "There are websites like: `https://www.youtube.com/watch?v=bi2QKY3zW8Q`\n",
        "\n",
        "(Extra: we do not have emails and the `@` not means email)\n",
        "\n",
        "There are outlaw words that repeats the same character more than twice: `MOOOOOOOOOOOOOOOOOOOOOOooooooooooooooooooooooooooooo...`, `sucks!!........Hmmmm`, `HAAHAHAHAHHAAHHAHAHHAH`, `Waipahoooooooooo????` and  `succcccccccccccccckkkkkk`. If you know the correct term for these elongated words, I would love to find out. Until then, we will refer them as ‘outlaw words’. These outlaw cases seem to appear very rarely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDjT9-56zy3W"
      },
      "source": [
        "#from https://stackoverflow.com/questions/42329766/python-nlp-british-english-vs-american-english\n",
        "\n",
        "# method that convert the British to American English\n",
        "def replace_all(text, mydict):\n",
        "    for gb, us in mydict.items():\n",
        "        text = text.replace(us, gb)\n",
        "    return text\n",
        "\n",
        "BvsA = json.load(open(\"bvsa.txt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlD51E2Pzy3X"
      },
      "source": [
        "us_text = replace_all(train_string, BvsA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_xa6sdIzy3X"
      },
      "source": [
        "# Check if we have NLP British English and American English\n",
        "if train_string == us_text:\n",
        "    print(\" equals \" )\n",
        "else:\n",
        "    print( \" Not equal, so we have British English and American English.\\n\"+\n",
        "           \" As a result, we may keep the American form (convert British to American English).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMOlJNtCzy3X"
      },
      "source": [
        "If we decide to keep the covert edition, it will need to change the positions of `spans`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC4g5duozy3Y"
      },
      "source": [
        "def summarise(pattern, strings, freq):\n",
        "    \"\"\"Summarise strings matching a pattern.\"\"\"\n",
        "    # Find matches\n",
        "    compiled_pattern = re.compile(pattern)\n",
        "    matches = [s for s in strings if compiled_pattern.search(s)]\n",
        "    \n",
        "    # Print volume and proportion of matches\n",
        "    print(\"{} strings, that is {:.2%} of total\".format(len(matches), len(matches)/ len(strings)))\n",
        "    \n",
        "    # Create list of tuples containing matches and their frequency\n",
        "    output = [(s, freq[s]) for s in set(matches)]\n",
        "    output.sort(key=lambda x:x[1], reverse=True)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI0pSKbpzy3Y"
      },
      "source": [
        "# count the frequent of numbers\n",
        "summarise(r\"\\d\", splits, freq_splits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDqkK1Zvzy3Z"
      },
      "source": [
        "It is frequent to have numbers (54%)\n",
        "\n",
        "We have a variety of numbers, percentage, numeric, money, date and others. But the numbers, as we said before, do not reveal toxicity. We may need to drop numbers when tokenising. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ1dJi5dzy3Z"
      },
      "source": [
        "# find the frequent of hyphenated words\n",
        "summarise(r\"\\w+-+\\w+\", splits, freq_splits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ine2zbCPzy3Z"
      },
      "source": [
        "It is frequent (44%), these ones definitely need to be separated.\n",
        "\n",
        "From glancing through the hyphenated words, it makes more sense to separate them out to keep the data simple. For example: We should tokenise ‘same-sex’ to 2 tokens: `[‘same’, ‘sex’]` instead of 1 token: `[‘same-sex’]`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8GxGO4bzy3Z"
      },
      "source": [
        "#count requent of words combined by other punctuation\n",
        "summarise(r\"\\w+[_!&/)(<\\|}{\\[\\]]\\w+\", splits, freq_splits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4MFAzCzy3a"
      },
      "source": [
        "Not too frequent, but it is important.\n",
        "\n",
        "We can observe that some toxic word hidden behind the punctuation. \n",
        "\n",
        "For instance, `'(D)onkeys'`->`'Donkeys'`, `fu/k`->`fuck`\n",
        "\n",
        "Furthermore, we have same similars words, which it seems to be unuseful, like `is/was`,`he/she` or `'him/her'`. In addition, we have same words in different series, like `is/was` and `was/is`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4muBfHBbzy3a"
      },
      "source": [
        "# count frequent of outlaw words\n",
        "def find_outlaw(word):\n",
        "    \"\"\"Find words that contain a same character 3+ times in a row.\"\"\"\n",
        "    is_outlaw = False\n",
        "    for i, letter in enumerate(word):\n",
        "        if i > 1:\n",
        "            if word[i] == word[i-1] == word[i-2] and word[i].isalpha():\n",
        "                is_outlaw = True\n",
        "                break\n",
        "    return is_outlaw\n",
        "\n",
        "outlaws = [s for s in splits if find_outlaw(s)]\n",
        "print(\"{} strings, that is {:.2%} of total\".format(len(outlaws), len(outlaws)/ len(splits)))\n",
        "outlaw_freq = [(s, freq_splits[s]) for s in set(outlaws)]\n",
        "outlaw_freq.sort(key=lambda x:x[1], reverse=True)\n",
        "outlaw_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNrA1yrmzy3a"
      },
      "source": [
        "Not too frequent, these may not worth correcting for because there too few cases. However, we will need to seperate, because it hidde toxic words like `killls`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtz7I17Dzy3b"
      },
      "source": [
        "# tokenise the text into alphabetic tokens\n",
        "tokeniser = RegexpTokenizer(\"[A-Za-z]+\")\n",
        "tokens = tokeniser.tokenize(train_string)\n",
        "print(tokens[:20], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBAqtkGdzy3c"
      },
      "source": [
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "print(f\"Number of unique tokens: {len(set(tokens))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxQ7fG2jzy3c"
      },
      "source": [
        "There are 300.000 tokens in the training data with around 23.000 unique tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDEa9V10zy3c"
      },
      "source": [
        "# convert all tokens into lowercase and lemmatise them\n",
        "nltk.download('wordnet')\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "tokens_norm = [lemmatiser.lemmatize(t.lower(), \"v\") for t in tokens]\n",
        "print(f\"Number of unique tokens: {len(set(tokens_norm))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW_ptjfrzy3c"
      },
      "source": [
        "temp = round(1 - len(set(tokens_norm))/len(set(tokens)), 2)\n",
        "print('We drop by about ' + str(temp* 100) + '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0R-un6yzy3d"
      },
      "source": [
        "**!caution! If we keep the lemmatise method, we should change the `spans`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jIIEjzzzy3d"
      },
      "source": [
        "# Create list of token lengths for each token\n",
        "token_length = [len(t) for t in tokens]\n",
        "\n",
        "# Average number of characters per token\n",
        "print(f\"Average number of characters per token: {round(np.mean(token_length),4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpzljORKzy3d"
      },
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.violinplot(token_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Z3ZHGszy3d"
      },
      "source": [
        "token_length_2 = [len(t) for t in tokens if len(t)<15]\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.violinplot(token_length_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AuhxcVozy3d"
      },
      "source": [
        "# Plot distribution\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.countplot(y=token_length)\n",
        "plt.title(\"Counts of token length\", size=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bjOiYTHzy3f"
      },
      "source": [
        "There are a few tokens that is very long but also very rare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozcqt2j9zy3f"
      },
      "source": [
        "# load stopwords and add some more\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words.extend([\"cannot\", \"could\", \"done\", \"let\", \"may\" \"mayn\",  \"might\", \"must\", \"need\", \"ought\", \"oughtn\", \"shall\", \"would\", \"br\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "httFOY70zy3f"
      },
      "source": [
        "# the most common stop words in our corpus\n",
        "freq_stopwords = [(sw, tokens_norm.count(sw)) for sw in stop_words]\n",
        "freq_stopwords.sort(key=lambda x: x[1], reverse=True)\n",
        "freq_stopwords[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVMZYtepzy3f"
      },
      "source": [
        "# count all the stopwords\n",
        "n_stopwords = len([t for t in tokens_norm if t in stop_words])\n",
        "print(f\"{n_stopwords} tokens are stop words.\")\n",
        "print(f\"The proportion of tokens are stop words is: {round(100*n_stopwords/len(tokens_norm),2)}%.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbzMOrBWzy3g"
      },
      "source": [
        "About half of the tokens are stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjWuCJyXzy3g"
      },
      "source": [
        "# remove stop words\n",
        "tokens_clean = [t for t in tokens_norm if t not in stop_words]\n",
        "print(f\"Number of tokens: {len(tokens_clean)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMq0zFgYzy3g"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text into normalised tokens.\"\"\"\n",
        "    # Tokenise words into alphabetic tokens\n",
        "    tokeniser = RegexpTokenizer(r'[A-Za-z]{2,}')\n",
        "    tokens = tokeniser.tokenize(text)\n",
        "    \n",
        "    # Lowercase and lemmatise \n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
        "    \n",
        "    # Remove stopwords\n",
        "    keywords= [lemma for lemma in lemmas if lemma not in stop_words]\n",
        "    return keywords\n",
        "\n",
        "def get_frequent_ngram(corpus, ngram, n=20):\n",
        "    \"\"\"Find most common n n-grams tokens.\"\"\"\n",
        "    # Preprocess each document\n",
        "    documents = [preprocess_text(document) for document in corpus]\n",
        "    \n",
        "    # Find ngrams per document\n",
        "    n_grams = [list(ngrams(document, ngram)) for document in documents]\n",
        "    \n",
        "    # Find frequency of ngrams\n",
        "    n_grams_flattened = [item for sublist in n_grams for item in sublist]\n",
        "    freq_dist = FreqDist(n_grams_flattened)\n",
        "    top_freq = freq_dist.most_common(n)\n",
        "    return pd.DataFrame(top_freq, columns=[\"ngram\", \"count\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ5mskGhzy3g"
      },
      "source": [
        "# Get frequent ngrams for all 4\n",
        "for i in range(1,5):\n",
        "    mapping = {1:\"uni\", 2:\"bi\", 3:\"tri\", 4:\"four\"}\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.barplot(x=\"count\", y=\"ngram\", data=get_frequent_ngram(dataset['text'], i))\n",
        "    plt.title(f\"Most common {mapping[i]}grams\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tPoqRrrzy3g"
      },
      "source": [
        "The word `trump` (logically from `trumpet` after lemm)and `stupid` looks quite frequent compared to the other frequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vGI914Mzy3g"
      },
      "source": [
        "def get_avg_word_len(x):\n",
        "    words = x.split()\n",
        "    word_len = 0\n",
        "    for word in words:\n",
        "        word_len = word_len + len(word)\n",
        "        \n",
        "    return word_len/len(words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOIaLFrszy3g"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "# tokeniser = RegexpTokenizer(\"[A-Za-z]+\")\n",
        "new_dataset = dataset.copy()\n",
        "\n",
        "#delete the  None (NA) types in your DF\n",
        "new_dataset.dropna(inplace=True)\n",
        "\n",
        "new_dataset[\"n_sentences\"] = new_dataset[\"text\"].apply(sent_tokenize).apply(len)\n",
        "new_dataset[\"tokens\"] = new_dataset[\"text\"].apply(tokeniser.tokenize)\n",
        "new_dataset[\"n_tokens\"] = new_dataset[\"tokens\"].apply(len)\n",
        "new_dataset[\"n_characters\"] = dataset[\"text\"].apply(len)\n",
        "new_dataset[\"n_stopwords\"] = new_dataset[\"tokens\"].apply(lambda tokens: len([t for t in tokens if t in stop_words]))\n",
        "new_dataset[\"p_stopwords\"] = new_dataset[\"n_stopwords\"]/new_dataset[\"n_tokens\"]\n",
        "\n",
        "# Inspect head\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVFsTok4zy3g"
      },
      "source": [
        "sns.distplot(new_dataset['n_tokens'],bins=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs2dd-bVzy3g"
      },
      "source": [
        "sns.distplot(new_dataset['n_characters'],bins=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOloGQVXzy3h"
      },
      "source": [
        "sns.pairplot(new_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jp8jlulzy3h"
      },
      "source": [
        "new_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viuhTHSjzy3h"
      },
      "source": [
        "bad_word_train = pd.DataFrame(all_bad_word, columns = ['toxic'])\n",
        "bad_word_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGOEtQ9Qzy3h"
      },
      "source": [
        "#delete the  None (NA) types in your DF\n",
        "bad_word_train.replace(\"\",float(\"NaN\") , inplace=True)\n",
        "bad_word_train = bad_word_train.dropna()\n",
        "\n",
        "bad_word_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R97BkiEazy3i"
      },
      "source": [
        "temp = ' '.join(bad_word_train.toxic.tolist())\n",
        "bad_word_train_clean = preprocess_text(temp) #call the function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M2yB9Lpzy3i"
      },
      "source": [
        "# freguent of toxic words in corpus\n",
        "bad_word_train_freq = FreqDist(bad_word_train_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZbWZDaszy3i"
      },
      "source": [
        "# keep the 50 most common words\n",
        "common_bad_word_train_freq = bad_word_train_freq.most_common(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR06tFvGzy3i"
      },
      "source": [
        "#FreqDist to Pandas dataframe\n",
        "df_bad_word_train_freq= pd.DataFrame(common_bad_word_train_freq, \n",
        "                                    columns = ['Term','Frequency'])\n",
        "df_bad_word_train_freq.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYgw670Kzy3j"
      },
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "sns.barplot(x=\"Frequency\", y=\"Term\", data=df_bad_word_train_freq)\n",
        "plt.title(\"Top toxic words in corpus\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edB4MVOIzy3j"
      },
      "source": [
        "# Ιnformation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ3qAU4bzy3j"
      },
      "source": [
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-sWCnx7zy3j"
      },
      "source": [
        "def tranform_dataset(dataset):\n",
        "    list_doc = [] # is the number of each row in dataset\n",
        "    list_tokens = [] # token of each text\n",
        "    list_span_start = [] # start position of token\n",
        "    list_span_label = [] # label initialize\n",
        "\n",
        "    # tranform the dataset as text to token\n",
        "    for text_i in tqdm(range(0,len(dataset['text']))):\n",
        "\n",
        "        list_doc.append(text_i)\n",
        "        \n",
        "        # tokenization by space \" \"\n",
        "        token = dataset.text[text_i].split()\n",
        "        list_tokens.append(token)\n",
        "\n",
        "        # code to find the start position of each word\n",
        "        start_pos = []\n",
        "        count = 0\n",
        "        for i in token:\n",
        "            start_pos.append(count)\n",
        "            count+=len(i)+1\n",
        "        list_span_start.append(start_pos)\n",
        "        \n",
        "        list_span_label.append([0]* len(start_pos))\n",
        "        \n",
        "    \n",
        "    # add the new list into a dataframe\n",
        "    df_token = pd.DataFrame(list_doc, columns=['doc'])\n",
        "    df_token['token'] = list_tokens\n",
        "    df_token['span_start'] = list_span_start\n",
        "    \n",
        "    df_token['toxic'] = list_span_label\n",
        "    \n",
        "    return df_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvZfNtnbzy3k"
      },
      "source": [
        "df_token = tranform_dataset(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV9aUgmszy3k"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUZfrgJKzy3l"
      },
      "source": [
        "df_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2w9mrfQzy3l"
      },
      "source": [
        "def detect_labels(df_token,dataset):   \n",
        "    # find toxic tokens\n",
        "    for i in tqdm(range(0,len(df_token.token))):\n",
        "\n",
        "        #eg span_dataset = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...\n",
        "        span_dataset = dataset.spans[i]\n",
        "        all_dataset = df_token.span_start[i]\n",
        "\n",
        "        pos = 0\n",
        "        for x in all_dataset:\n",
        "            if x in span_dataset:\n",
        "                df_token.toxic[i][pos]=1\n",
        "            pos+=1\n",
        "           \n",
        "    return df_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9HXk-7Dzy3l"
      },
      "source": [
        "df = detect_labels(df_token,dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDUQjhwMzy3l"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7FEZA5Qzy3m"
      },
      "source": [
        "df.iloc[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WieLGS5Zzy3m"
      },
      "source": [
        "# where to save it, usually as a .pkl\n",
        "df.to_pickle('myDataset_training')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_JlwsKVzy3m"
      },
      "source": [
        "**Same code for develop/validation dataset `tsd`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6noVIVpTzy3m"
      },
      "source": [
        "df_dev = tranform_dataset(tsd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyu4B2cQzy3m"
      },
      "source": [
        "df_dev = detect_labels(df_dev,tsd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV6YCNjxzy3m"
      },
      "source": [
        "df_dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duyPlojVzy3n"
      },
      "source": [
        "for i in range(len(df_dev.doc)):\n",
        "    if df_dev.doc[i]==8:\n",
        "        print(df_dev.toxic[i])\n",
        "        print(df_dev.token[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tskvTz1zy3n"
      },
      "source": [
        "# where to save it, usually as a .pkl\n",
        "df_dev.to_pickle('myDataset_develop') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZWHeAsOzy3n"
      },
      "source": [
        "# Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYw1427tzy3n"
      },
      "source": [
        "def clean(df_train_clean): \n",
        "\n",
        "    #lowercase\n",
        "    df_train_clean['token'] = df_train_clean['token'].str.lower()\n",
        "    \n",
        "    #Fisrt and foremost we delete \\n , because we have \\nDon't\n",
        "    df_train_clean['token'].replace('\\n', '' , inplace=True)\n",
        "    \n",
        "    #remove `?`\n",
        "    df_train_clean['token'].replace(re.compile(r'[?]'), '', inplace=True)\n",
        "\n",
        "    # remove specific char\n",
        "    df_train_clean['token'].replace(re.compile(r'[)\"(,:‘}{=><]'), '', inplace=True) #correct\n",
        "    \n",
        "    # tranform percentage to TAG (eg 34% )\n",
        "    df_train_clean['token'].replace(re.compile(r'\\d+[%]'), 'NUM', inplace=True)\n",
        "    \n",
        "    # tranform money to TAG (eg 34$ )\n",
        "    df_train_clean['token'].replace(re.compile(r'\\d+[$]'), 'MONEY', inplace=True)\n",
        "    \n",
        "    # tranform money to TAG (eg $34)\n",
        "    df_train_clean['token'].replace(re.compile(r'[$]\\d+'), 'MONEY', inplace=True)\n",
        "    \n",
        "    # tranform money to TAG (eg $34)\n",
        "    df_train_clean['token'].replace(re.compile(r'[$]\\w+'), 'TOXIC', inplace=True)\n",
        "    \n",
        "    # tranform multi full stop to one full stop (eg MAm....... )\n",
        "    df_train_clean['token'].replace(re.compile(r'[.]+'), '.', inplace=True)\n",
        "    \n",
        "    #remove `.`\n",
        "    df_train_clean['token'].replace(re.compile(r'[.]'), '', inplace=True)\n",
        "    \n",
        "    # change hide toxic words (e.g f@@k = f**k)\n",
        "    df_train_clean['token'].replace(re.compile(r\"(\\w+[*+$#@&%!]+\\w+)\"), 'TOXIC', inplace=True)\n",
        "    \n",
        "    # change hide toxic words (e.g f***)\n",
        "    df_train_clean['token'].replace(re.compile(r\"(\\w+[*$#@&%])\"), 'TOXIC', inplace=True)\n",
        "    \n",
        "    #remove `.`\n",
        "    df_train_clean['token'].replace(re.compile(r'[!]'), '', inplace=True)\n",
        "    \n",
        "    # tranform reference to TAG\n",
        "    df_train_clean['token'].replace(re.compile(r'@(\\w+)'), 'REF' , inplace=True)\n",
        "    \n",
        "    #Remove repeating characters from words (eg. my loveeeee)\n",
        "    df_train_clean['token'].replace(to_replace=r'(.)\\1\\1+',  value=r'\\1', regex=True, inplace=True)\n",
        "    \n",
        "\n",
        "    # tranform urls to TAG   #help for: https://www.geeksforgeeks.org/python-check-url-string/\n",
        "    df_train_clean['token'].replace(re.compile(r'(https?://[^\\s]+)'), 'URL' , inplace=True)    \n",
        "\n",
        "\n",
        "    #remove emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    df_train_clean['token'].replace(emoji_pattern, 'EMO' , inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return df_train_clean['token']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcqjurWOzy3o"
      },
      "source": [
        "#clean dataset code\n",
        "\n",
        "df_train_clean = df.copy()\n",
        "for doc in tqdm(range(len(df_train_clean.token))):\n",
        "    temp = pd.DataFrame(df_train_clean.token[doc],columns=['token'])\n",
        "    df_train_clean.token[doc] = clean(temp).values.tolist()\n",
        "    \n",
        "df_develop_clean = df_dev.copy()\n",
        "for doc in tqdm(range(len(df_develop_clean.token))):\n",
        "    temp = pd.DataFrame(df_develop_clean.token[doc],columns=['token'])\n",
        "    df_develop_clean.token[doc] = clean(temp).values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RW0xImEzy3o"
      },
      "source": [
        "df_train_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nz2bAqWzy3o"
      },
      "source": [
        "# System 1 (with LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENcKauT9zy3p"
      },
      "source": [
        "## Data Pre-processing for Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux2AzY4rzy3p"
      },
      "source": [
        "def flatten(input):\n",
        "    new_list = []\n",
        "    new_doc = []\n",
        "    count_doc = 0\n",
        "    for i in input:\n",
        "        for j in i:\n",
        "            new_list.append(j)\n",
        "            new_doc.append(str(count_doc))\n",
        "        count_doc+=1\n",
        "    return new_list, new_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TfbsJKEzy3q"
      },
      "source": [
        "flat,doc_ids = flatten(df_train_clean.token.values) \n",
        "labels,doc_ids = flatten(df_train_clean.toxic.values)\n",
        "\n",
        "df_train = pd.DataFrame(flat, columns= ['token'])\n",
        "df_train['toxic'] = labels\n",
        "df_train['doc'] = doc_ids\n",
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NaEKqBHzy3q"
      },
      "source": [
        "flat,doc_ids = flatten(df_develop_clean.token.values)\n",
        "labels,doc_ids = flatten(df_develop_clean.toxic.values)\n",
        "\n",
        "df_develop = pd.DataFrame(flat, columns= ['token'])\n",
        "df_develop['toxic'] = labels\n",
        "df_develop['doc'] = doc_ids\n",
        "df_develop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8WPcPoFzy3r"
      },
      "source": [
        "for i in range(len(df_develop.doc)):\n",
        "    if df_develop.doc[i]=='3':\n",
        "        print(df_develop.token[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6cDlPZbzy3r"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN4wl1E5zy3r"
      },
      "source": [
        "from https://github.com/ferrygun/NERC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Uvsp41zy3r"
      },
      "source": [
        "words = list(set(df_train[\"token\"].values))\n",
        "words.append(\"UNK\") #georg\n",
        "words.append(\"ENDPAD\")\n",
        "\n",
        "words_val = list(set(df_develop[\"token\"].values))\n",
        "words_val.append(\"ENDPAD\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrkZ0Lowzy3s"
      },
      "source": [
        "n_words = len(words)\n",
        "print('Number of words(=tokens)in training dataset: '+str(n_words))\n",
        "\n",
        "n_words_val = len(words_val)\n",
        "print('Number of words(=tokens) in validation dataset: '+str(n_words_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3B9vF2Kzy3s"
      },
      "source": [
        "tags = list(set(df_train[\"toxic\"].values))\n",
        "n_tags = len(tags)\n",
        "print('Number of tags : '+str(n_tags)+ ' = ' +str(tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmm-LU1zzy3s"
      },
      "source": [
        "## Features Extraction on Sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWpGC475zy3s"
      },
      "source": [
        "In preparation for features extraction, we create the following helpers:\n",
        "\n",
        "`word2idx`: convert word to index\n",
        "\n",
        " `tag2idx`: convert tag to index\n",
        " \n",
        " `idx2word`: convert index to word\n",
        " \n",
        " `idx2tag`: convert index to tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLhFmbu3zy3s"
      },
      "source": [
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvANhkACzy3t"
      },
      "source": [
        "len(word2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KwBvOvMzy3t"
      },
      "source": [
        "**Solve the problem out of vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwxuRNkUzy3t"
      },
      "source": [
        "df_develop_old = df_develop.copy()\n",
        "\n",
        "for w in tqdm(words_val):\n",
        "    if w not in word2idx:\n",
        "        df_develop.token.replace(w,'UNK', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFuQQr_Pzy3t"
      },
      "source": [
        "# print new alternative sentence\n",
        "for i in range(len(df_develop.doc)):\n",
        "    if df_develop.doc[i]=='3':\n",
        "        print(df_develop.token[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGgs0wRhzy3u"
      },
      "source": [
        "We write SentenceGetter class to retrieve sentences with their words, POS and tags from the datasets. The code is taken from https://www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ou8YZ1zy3u"
      },
      "source": [
        "def mySentenceGetter(data):\n",
        "    doc_id = 0\n",
        "    sentences = []\n",
        "    temp_doc = []\n",
        "    for i in tqdm(range(len(data.doc))):\n",
        "        if data.doc[i]==doc_id :\n",
        "            temp_doc.append((data[\"token\"][i], data[\"toxic\"][i]))\n",
        "        else:\n",
        "            sentences.append(temp_doc)\n",
        "            doc_id = data.doc[i]\n",
        "            temp_doc = []\n",
        "            temp_doc.append((data[\"token\"][i], data[\"toxic\"][i]))\n",
        "    sentences.append(temp_doc)\n",
        "    return sentences[1:]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVbyDvZSzy3u"
      },
      "source": [
        "sentences = mySentenceGetter(df_train)\n",
        "\n",
        "sentences_val = mySentenceGetter(df_develop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y0SX9Xtzy3u"
      },
      "source": [
        "We need to convert the sentences to numbers as Keras work better:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsnQkiTIzy3u"
      },
      "source": [
        "X_train = [[word2idx[w[0]] for w in s] for s in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaQBGBBHzy3u"
      },
      "source": [
        "X_test_prod = [[word2idx[w[0]] for w in s] for s in sentences_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KblEa1nTzy3v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHz8msPszy3v"
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg7ud7zzzy3v"
      },
      "source": [
        "for i in X_train[0]:\n",
        "    print(idx2word[i],end= \" \") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg0iuQbzzy3v"
      },
      "source": [
        "for i in X_test_prod[0]:\n",
        "    print(idx2word[i],end= \" \") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1o8IOvgzy3v"
      },
      "source": [
        "print(X_test_prod[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohy0Dpbszy3w"
      },
      "source": [
        "idx2word[n_words-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHeH4HHvzy3w"
      },
      "source": [
        "Padding: The LSTM layers accept sequences of same length only. Therefore we will want to transform our list of token_sequences which is lists of integers into a matrix of shape (token_sequences, max_len). We can use any length as max_len. In this project we will be using length of the longest sequence as max_len. The sequences that are shorter than max_len are padded with a specified value at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtovfIhJzy3w"
      },
      "source": [
        "We padding the sentences to the same length (max_len) and fill with index value of “ENDPAD” or 23614."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAD1ikt7zy3w"
      },
      "source": [
        "# find longest sentence\n",
        "max_len = max([len(s) for s in idx2word.values()])\n",
        "max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5my7K8Vzy3w"
      },
      "source": [
        "X_train = pad_sequences(maxlen=max_len, sequences=X_train, padding=\"post\",\n",
        "                  value=n_words - 1)\n",
        "\n",
        "X_test_prod = pad_sequences(maxlen=max_len, sequences=X_test_prod, padding=\"post\",\n",
        "                  value=n_words - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxu_SxVQzy3x"
      },
      "source": [
        "print('Result after padding')\n",
        "X_test_prod[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59-2hF5Uzy3x"
      },
      "source": [
        "for k in X_test_prod[4]:    \n",
        "    print(idx2word[k],end= \" \") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7nWdXLozy3x"
      },
      "source": [
        "#### Labels\n",
        "Convert labels (tags) to numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hopaBl82zy3x"
      },
      "source": [
        "Padding the labels (we padding the Labels to the same length (max_len) and fill with index value of entity `non` or `0`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dodw-Bfuzy3x"
      },
      "source": [
        "y_train = [[w[1] for w in s] for s in sentences]\n",
        "\n",
        "y_test_prod = [[w[1] for w in s] for s in sentences_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1ksu5Cqzy3x"
      },
      "source": [
        "y_train = pad_sequences(maxlen=max_len, sequences=y_train, \n",
        "                  padding=\"post\", value=2)###value=0 ->value=2\n",
        "\n",
        "y_test_prod = pad_sequences(maxlen=max_len, sequences=y_test_prod, \n",
        "                  padding=\"post\", value=2)###value=0->value=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rNz_6brzy3y"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLUQ6EmHzy3y"
      },
      "source": [
        "tag2idx['ENDPAD']= 2###\n",
        "idx2tag[2]= 'ENDPAD'###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o71tO0Ghzy3y"
      },
      "source": [
        "#update number of classes\n",
        "n_tags = len(tag2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_d6c_YIzy3y"
      },
      "source": [
        "Perform One-Hot Encoding for labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSiOfjXOzy3y"
      },
      "source": [
        "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train] \n",
        "\n",
        "y_test_prod = [to_categorical(i, num_classes=n_tags) for i in y_test_prod]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZdQ3b54zy3y"
      },
      "source": [
        "y_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d224F4_Uzy3z"
      },
      "source": [
        "## Neural Network Model (LSTM)\n",
        "\n",
        "We create and compile the sequential model with the following architecture.\n",
        "\n",
        "RNNs are capable of handling different input and output combinations. We will use many to many architectures for this ask. Refer to the last architecture in the image given below. Our task is to output tag (y) for a token (X) ingested at each time step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q65Bgu3Czy3z"
      },
      "source": [
        "![title](images/rnn.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znpatJX1zy3z"
      },
      "source": [
        "form from https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\n",
        "\n",
        "We are building a simple model with 3 layers.\n",
        "\n",
        "- **Layer 1 - Embedding layer** : We will feed the padded sequences of equal lengthto the embedding layer. Once the network has been trained, each token will get transformed into a vector of n dimensions. We have chosen the n dimensions to be. \n",
        "\n",
        "\n",
        "- **Layer 2 - Bidirectional LSTM** : Bidirectional lstm takes a recurrent layer (e.g. the first LSTM layer) as an argument. This layer takes the output from the previous embedding layer. It also allows you to specify the merge mode, that is how the forward and backward outputs should be combined before being passed on to the next layer. The default mode is to concatenate, where the outputs are concatenated together, providing double the number of outputs to the next layer.\n",
        "\n",
        "\n",
        "- **Layer  LSTM Layer** (at end we do not put): An LSTM network is a recurrent neural network that has LSTM cell blocks in place of our standard neural network layers. These cells have various components called the input gate, the forget gate and the output gate. (at end we do not put)\n",
        "\n",
        "\n",
        "\n",
        "- **Layer 3- TimeDistributed  Layer** : We are dealing with Many to Many RNN Architecture where we expect output from every input sequence for example (a1 →b1, a2 →b2… an →bn) where a and b are inputs and outputs of every sequence. The TimeDistributeDense layers allow you to apply Dense(fully-connected) operation across every output over every time-step. If you don't use this, you would only have one final output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3gCI_aJzy3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a85af4-9518-4106-ad7d-e4fd1abf256b"
      },
      "source": [
        "input = Input(shape=(max_len,))\n",
        "\n",
        "# Add Embedding layer\n",
        "model = Embedding(input_dim=n_words, output_dim=50, \n",
        "                  input_length=max_len, mask_zero=True)(input)\n",
        "\n",
        "model = Dropout(0.5)(model)\n",
        "\n",
        "# Add bidirectional LSTM\n",
        "model = Bidirectional(LSTM(units=100, return_sequences=True,\n",
        "                           recurrent_dropout=0.1))(model)\n",
        "# Add timeDistributed Layer\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"sigmoid\"))(model) ###n_tags not(n_tags+1)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umUomjkHzy3z"
      },
      "source": [
        "model = Model(input, out)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cnuc-7rzy30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7225366e-868c-466d-84a1-d6bde4d8d416"
      },
      "source": [
        "#Optimiser \n",
        "opt = tensorflow.keras.optimizers.Adam(lr=0.01, decay=1e-6)\n",
        "\n",
        "model.compile(optimizer=opt, loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])  "
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCaMH82_zy30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e103871-4f79-4515-8588-5aa963526ec5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 125)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 125, 50)           248300    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 125, 50)           0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 125, 200)         120800    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 125, 3)           603       \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 369,703\n",
            "Trainable params: 369,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NNh0zDMzy30"
      },
      "source": [
        "early = EarlyStopping(monitor=\"val_loss\", \n",
        "                              mode=\"min\" if \"loss\" in \"val_loss\" else \"max\", \n",
        "                              patience=1, verbose=1,\n",
        "                              min_delta=0.0001, restore_best_weights=True)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dddluU67zy30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1271317d-fc5b-48ed-f782-82ba3f1255d3"
      },
      "source": [
        "# training mode\n",
        "history = model.fit(X_train, np.array(y_train), \n",
        "                    batch_size=32, \n",
        "                    epochs=10, \n",
        "                    validation_split=0.1,\n",
        "                    verbose=1, \n",
        "                    callbacks=[early])"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 38s 1s/step - loss: 0.2623 - accuracy: 0.9265 - val_loss: 0.0882 - val_accuracy: 0.9759\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 29s 1s/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0738 - val_accuracy: 0.9759\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 29s 1s/step - loss: 0.0525 - accuracy: 0.9836 - val_loss: 0.0709 - val_accuracy: 0.9761\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9849Restoring model weights from the end of the best epoch: 3.\n",
            "20/20 [==============================] - 29s 1s/step - loss: 0.0445 - accuracy: 0.9849 - val_loss: 0.1078 - val_accuracy: 0.9763\n",
            "Epoch 00004: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u_HwGEszy31"
      },
      "source": [
        "#### Plot Accuracy and Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NcvZkD87zy31"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k130wPHazy31"
      },
      "source": [
        "***Initialize***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBe56s5dzy31"
      },
      "source": [
        "toxic_label = 1\n",
        "not_toxic_label = 0\n",
        "class_num = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDjv6mbpzy31"
      },
      "source": [
        "**F1 function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVLDLKHPzy31"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set) + len(gold_set)\n",
        "    return float(nom)/float(denom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUbXet3Nzy32"
      },
      "source": [
        "**Get offset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Oyd5Brzy32"
      },
      "source": [
        "threshold = 0.18 #by user test\n",
        "\n",
        "text_predictions = model.predict(X_test_prod)\n",
        "\n",
        "my_pred = []\n",
        "for i in tqdm(range(len(text_predictions))):\n",
        "    max_prob = 0\n",
        "    tmp = []\n",
        "    for j in range(len(text_predictions[i])):\n",
        "        #if find class pad, stop    \n",
        "        if text_predictions[i][j][2]>0.8:\n",
        "            break\n",
        "         \n",
        "        if text_predictions[i][j][1]>threshold:\n",
        "            tmp.append(1)\n",
        "        else:\n",
        "            tmp.append(0)\n",
        "            \n",
        "        \n",
        "    my_pred.append(tmp)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VNApAw_zy32"
      },
      "source": [
        "df_temp = pd.DataFrame({'pred':my_pred})\n",
        "df_temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuQvEJqtzy32"
      },
      "source": [
        "toxic_offsets_all=[]\n",
        "for i in range(len(df_dev.token)):\n",
        "    toxic_offsets = []\n",
        "    current_offset = 0\n",
        "    for j in range(len(df_dev.token[i])):\n",
        "        if df_temp.pred[i][j]==1:\n",
        "            toxic_offsets.extend(list(range(current_offset, current_offset+len(df_dev.token[i][j]))))\n",
        "        current_offset += len(df_dev.token[i][j]) + 1\n",
        "    toxic_offsets_all.append(toxic_offsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liPVFMKVzy33"
      },
      "source": [
        "df_predict = pd.DataFrame({'pred':toxic_offsets_all})\n",
        "df_predict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9psHNrGNzy33"
      },
      "source": [
        "tsd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz9aC4Pezy33"
      },
      "source": [
        "tsd[\"predictions\"] = df_predict.pred\n",
        "tsd[\"f1_scores\"] = tsd.apply(lambda row: f1(row.predictions, row.spans), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QQXu0PNzy33"
      },
      "source": [
        "tsd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nERgnMp2zy34"
      },
      "source": [
        "from scipy.stats import sem\n",
        "print (f\"F1 = {tsd.f1_scores.mean():.2f} ± {sem(tsd.f1_scores):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64jiCIblzy34"
      },
      "source": [
        "_ = tsd.f1_scores.plot(kind=\"box\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETSut-qpzy34"
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHwE3fFQzy35"
      },
      "source": [
        "## Test set Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIAJKRR4zy35"
      },
      "source": [
        "# test data\n",
        "dataset_test = pd.read_csv(\"data/tsd_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71hL9KpTzy35"
      },
      "source": [
        "dataset_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMRMmUJezy35"
      },
      "source": [
        "list_doc = [] # is the number of each row in dataset\n",
        "list_tokens = [] # token of each text\n",
        "\n",
        "# tranform the dataset as text to token\n",
        "for text_i in tqdm(range(0,len(dataset_test['text']))):\n",
        "\n",
        "    list_doc.append(text_i)\n",
        "        \n",
        "    # tokenization by space \" \"\n",
        "    token = dataset_test.text[text_i].split()\n",
        "    list_tokens.append(token)\n",
        "\n",
        "    # add the new list into a dataframe\n",
        "    df_test_start = pd.DataFrame(list_doc, columns=['doc'])\n",
        "    df_test_start['token'] = list_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM_2veQFzy35"
      },
      "source": [
        "df_test_start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpeb08SEzy36"
      },
      "source": [
        "#clean dataset code\n",
        "df_test_start_clean = df_test_start.copy()\n",
        "for doc in tqdm(range(len(df_test_start_clean.token))):\n",
        "    temp = pd.DataFrame(df_test_start_clean.token[doc],columns=['token'])\n",
        "    df_test_start_clean.token[doc] = clean(temp).values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhz0ZtsIzy36"
      },
      "source": [
        "df_test_start_clean "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5IQ3CkCzy36"
      },
      "source": [
        "flat,doc_ids = flatten(df_test_start_clean.token) #clean dataset\n",
        "\n",
        "df_test = pd.DataFrame(flat, columns= ['token'])\n",
        "df_test['doc'] = doc_ids\n",
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z_o5TeNzy36"
      },
      "source": [
        "words_test = list(set(df_test[\"token\"].values))\n",
        "words_test.append(\"ENDPAD\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q88PZ_p7zy37"
      },
      "source": [
        "for w in tqdm(words_test):\n",
        "    if w not in word2idx:\n",
        "        df_test.token.replace(w,'UNK', inplace=True)\n",
        "        print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nOD-KVOzy37"
      },
      "source": [
        "def mySentenceGetter_test(data):\n",
        "    doc_id = 0\n",
        "    sentences = []\n",
        "    temp_doc = []\n",
        "    for i in tqdm(range(len(data.doc))):\n",
        "        if data.doc[i]==doc_id :\n",
        "            temp_doc.append((data[\"token\"][i]))\n",
        "        else:\n",
        "            sentences.append(temp_doc)\n",
        "            doc_id = data.doc[i]\n",
        "            temp_doc = []\n",
        "            temp_doc.append((data[\"token\"][i]))\n",
        "    sentences.append(temp_doc)\n",
        "    return sentences[1:] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWRfBDaXzy37"
      },
      "source": [
        "sentences_test = mySentenceGetter_test(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDwopZQVzy37"
      },
      "source": [
        "X_test = [[word2idx[w] for w in s] for s in sentences_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnpDB9GTzy37"
      },
      "source": [
        "X_test = pad_sequences(maxlen=max_len, sequences=X_test, padding=\"post\",\n",
        "                  value=n_words - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU4rJuZMzy37"
      },
      "source": [
        "text_predictions = model.predict(X_test)\n",
        "\n",
        "my_pred = []\n",
        "for i in tqdm(range(len(text_predictions))):\n",
        "    max_prob = 0\n",
        "    tmp = []\n",
        "    for j in range(len(text_predictions[i])):\n",
        "        #if find class pad, stop    \n",
        "        if text_predictions[i][j][2]>0.8:\n",
        "            break\n",
        "            \n",
        "        if text_predictions[i][j][1]>threshold:\n",
        "            tmp.append(1)\n",
        "        else:\n",
        "            tmp.append(0)\n",
        "            \n",
        "        \n",
        "    my_pred.append(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PvHyAI5zy38"
      },
      "source": [
        "df_temp = pd.DataFrame({'pred':my_pred})\n",
        "df_temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7zegrTVzy38"
      },
      "source": [
        "toxic_offsets_all=[]\n",
        "for i in range(len(df_test_start.token)):\n",
        "    toxic_offsets = []\n",
        "    current_offset = 0\n",
        "    for j in range(len(df_test_start.token[i])):\n",
        "        if df_temp.pred[i][j]==1:\n",
        "            toxic_offsets.extend(list(range(current_offset, current_offset+len(df_test_start.token[i][j]))))\n",
        "        current_offset += len(df_test_start.token[i][j]) + 1\n",
        "    toxic_offsets_all.append(toxic_offsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7YyHfwmzy38"
      },
      "source": [
        "df_predict = pd.DataFrame({'pred':toxic_offsets_all})\n",
        "df_predict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRCRP_3Pzy38"
      },
      "source": [
        "predictions = df_predict.pred.to_list()\n",
        "ids = df_predict.index.to_list()\n",
        "\n",
        "# write in a prediction file named \"spans-pred.txt\"\n",
        "with open(\"spans-pred.txt\", \"w\") as out:\n",
        "      for uid, text_scores in zip(ids, predictions):\n",
        "            out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTAccstezy39"
      },
      "source": [
        "! zip -r random_predictions.zip ./spans-pred.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nix75VSzy39"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rjzNZh4zy39"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH-WRcd_zy39"
      },
      "source": [
        "# System 2 (with LSTM-CRFs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glIfJCZIzy39"
      },
      "source": [
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras_contrib.layers import CRF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byWPhjExzy3-"
      },
      "source": [
        "input = Input(shape=(max_len,))\n",
        "model_crf = Embedding(input_dim=n_words + 1, output_dim=20,\n",
        "                  input_length=max_len)(input)  # 20-dim embedding\n",
        "model_crf = Bidirectional(LSTM(units=50, return_sequences=True,\n",
        "                           recurrent_dropout=0.1))(model_crf)  # variational biLSTM\n",
        "model_crf = TimeDistributed(Dense(50, activation=\"relu\"))(model_crf)  # a dense layer as suggested by neuralNer\n",
        "crf = CRF(n_tags)  # CRF layer\n",
        "out = crf(model_crf)  # output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RML2BACAzy3-"
      },
      "source": [
        "model_crf = Model(input, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdNO-VYPzy3-"
      },
      "source": [
        "model_crf.compile(optimizer=\"rmsprop\", loss= crf.loss_function,\n",
        "              metrics=[crf.accuracy])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Yjqq-mzy3-"
      },
      "source": [
        "# training\n",
        "history = model_crf.fit(X_train, np.array(y_train), \n",
        "                    batch_size=32, \n",
        "                    epochs=10, \n",
        "                    validation_split=0.1,\n",
        "                    verbose=1, \n",
        "                    callbacks=[early])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtNgL4dczy3-"
      },
      "source": [
        "threshold = 0.18 #by user test\n",
        "\n",
        "text_predictions = model_crf.predict(X_test_prod)\n",
        "\n",
        "my_pred = []\n",
        "for i in tqdm(range(len(text_predictions))):\n",
        "    max_prob = 0\n",
        "    tmp = []\n",
        "    for j in range(len(text_predictions[i])):\n",
        "        #if find class pad, stop    \n",
        "        if text_predictions[i][j][2]>0.8:\n",
        "            break\n",
        "         \n",
        "        if text_predictions[i][j][1]>threshold:\n",
        "            tmp.append(1)\n",
        "        else:\n",
        "            tmp.append(0)\n",
        "            \n",
        "        \n",
        "    my_pred.append(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2yd706zzy3_"
      },
      "source": [
        "df_temp = pd.DataFrame({'pred':my_pred})\n",
        "df_temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvKWIBOvzy3_"
      },
      "source": [
        "toxic_offsets_all=[]\n",
        "for i in range(len(df_dev.token)):\n",
        "    toxic_offsets = []\n",
        "    current_offset = 0\n",
        "    for j in range(len(df_dev.token[i])):\n",
        "        if df_temp.pred[i][j]==1:\n",
        "            toxic_offsets.extend(list(range(current_offset, current_offset+len(df_dev.token[i][j]))))\n",
        "        current_offset += len(df_dev.token[i][j]) + 1\n",
        "    toxic_offsets_all.append(toxic_offsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR3SJ2zNzy3_"
      },
      "source": [
        "df_predict = pd.DataFrame({'pred':toxic_offsets_all})\n",
        "df_predict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f1RBxk0zy3_"
      },
      "source": [
        "tsd[\"predictions_crf\"] = df_predict.pred\n",
        "tsd[\"f1_scores_crf\"] = tsd.apply(lambda row: f1(row.predictions_crf, row.spans), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq8kDC7yzy3_"
      },
      "source": [
        "tsd[:25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIRuZfLBzy3_"
      },
      "source": [
        "from scipy.stats import sem\n",
        "print (f\"F1 = {tsd.f1_scores_crf.mean():.2f} ± {sem(tsd.f1_scores_crf):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8kOK__gzy4A"
      },
      "source": [
        "_ = tsd.f1_scores.plot(kind=\"box\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtPrQpTlzy4A"
      },
      "source": [
        "model_crf.save(\"model_crf.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wciNFB9nzy4A"
      },
      "source": [
        "**Join Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZGilQU4zy4A"
      },
      "source": [
        "count_crf =0\n",
        "count=0\n",
        "for i in range(len(tsd.f1_scores_crf)):\n",
        "\n",
        "    if tsd.predictions_crf[i]==[]:\n",
        "\n",
        "        if tsd.f1_scores_crf[i]>=tsd.f1_scores[i]:\n",
        "            count_crf+=1\n",
        "        else:\n",
        "            count+=1\n",
        "        \n",
        "print(count_crf)\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96FitB6Rzy4A"
      },
      "source": [
        "tsd[\"predictions_join\"] = df_predict.pred\n",
        "for i in range(len(tsd[\"predictions_join\"])):\n",
        "    if tsd.predictions_crf[i]==[] and tsd.predictions[i]==[]:\n",
        "        tsd[\"predictions_join\"][i]=[]\n",
        "        next\n",
        "    if tsd[\"predictions\"][i]==[]:\n",
        "        tsd[\"predictions_join\"][i]=tsd[\"predictions_crf\"][i]\n",
        "    if tsd[\"predictions_crf\"][i]==[]:\n",
        "        tsd[\"predictions_join\"][i]=tsd[\"predictions\"][i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu7fqzMrzy4B"
      },
      "source": [
        "tsd[\"f1_scores_join\"] = tsd.apply(lambda row: f1(row.predictions_join, row.spans), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3HKLGCbzy4B"
      },
      "source": [
        "print (f\"F1 = {tsd.f1_scores_join.mean():.2f} ± {sem(tsd.f1_scores_join):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8k05eSYzy4B"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqnuBmf8zy4B"
      },
      "source": [
        "# System 3 (with sklearn)\n",
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnMTV-1bzy4B"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm.notebook import tqdm\n",
        "from ast import literal_eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89ce0QWwzy4B"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAK_Ud9fzy4C"
      },
      "source": [
        "df_develop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBOtwhhdzy4C"
      },
      "source": [
        "***Create one hot dataframe***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgqYltmszy4C"
      },
      "source": [
        "# find toxic words after cleaning text\n",
        "toxic_words_list = []\n",
        "nontoxic_words_list = []\n",
        "for i in tqdm(range(len(df_train.toxic))):\n",
        "    if df_train.toxic[i]==1:\n",
        "        toxic_words_list.append(df_train.token[i])\n",
        "        print(df_train.token[i])\n",
        "    else:\n",
        "        nontoxic_words_list.append(df_train.token[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYIutbrzy4C"
      },
      "source": [
        "len(toxic_words_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcrgNh9Jzy4C"
      },
      "source": [
        "# put toxic words in dataframe\n",
        "df_fast_1= pd.DataFrame(toxic_words_list,columns=['token_toxic'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NP1sG2Yzy4C"
      },
      "source": [
        "# transform to one hot\n",
        "df_fast = pd.get_dummies(df_fast_1.token_toxic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tSqg5I_zy4C"
      },
      "source": [
        "df_fast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zY2ohcLzy4D"
      },
      "source": [
        "df_y_1 = pd.DataFrame(1, index=np.arange(len(df_fast)),columns= ['toxic'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Ay1XxAzy4D"
      },
      "source": [
        "df_fast_2 = pd.DataFrame(0, index=np.arange(len(nontoxic_words_list)), columns=list(df_fast.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPNwK8aEzy4D"
      },
      "source": [
        "df_fast_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGPgcETHzy4D"
      },
      "source": [
        "df_y_2 = pd.DataFrame(0, index=np.arange(len(df_fast_2)),columns= ['toxic'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyMwDaNwzy4D"
      },
      "source": [
        "frames = [df_fast, df_fast_2]\n",
        "df_train_oneHot = pd.concat(frames)\n",
        "\n",
        "frames = [df_y_1,df_y_2]\n",
        "df_y = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBjYfPlHzy4D"
      },
      "source": [
        "df_train_oneHot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gTdZJtyzy4D"
      },
      "source": [
        "y = df_y.toxic.values\n",
        "classes = np.unique(y)\n",
        "classes = classes.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoUp-Wxazy4D"
      },
      "source": [
        "X_train = df_train_oneHot\n",
        "y_train = y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPlKTbxAzy4E"
      },
      "source": [
        "**Models from sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFd03ul7zy4E"
      },
      "source": [
        "sgd = SGDClassifier()\n",
        "sgd.partial_fit(X_train, y_train, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkESvArxzy4E"
      },
      "source": [
        "nb = MultinomialNB(alpha=0.01)\n",
        "nb.partial_fit(X_train, y_train, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpNEihB2zy4E"
      },
      "source": [
        "pa =PassiveAggressiveClassifier()\n",
        "pa.partial_fit(X_train, y_train, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuu0xOo6zy4E"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eo1D-zqzy4E"
      },
      "source": [
        "words = list(set(df_train[\"token\"].values))\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "\n",
        "words_val = list(set(df_develop[\"token\"].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSRksk19zy4E"
      },
      "source": [
        "df_develop_old = df_develop.copy()\n",
        "\n",
        "for w in tqdm(words_val):\n",
        "    if w not in word2idx:\n",
        "        df_develop.token.replace(w,'UNK', inplace=True) #georg\n",
        "#         #find the most similar words for those that are out of vocabulary\n",
        "#         similar = ft_model.wv.most_similar(positive=[w], topn=1)\n",
        "#         df_develop.token.replace(w,similar[0][0], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K90Jqs3-zy4F"
      },
      "source": [
        "df_develop['temp_toxic']= [0]*len(df_develop.token)\n",
        "df_develop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efmLZglpzy4F"
      },
      "source": [
        "for i in tqdm(range(len(df_develop.token))):\n",
        "    if df_develop.token[i] in toxic_words_list:\n",
        "         df_develop.temp_toxic[i]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWcoUimuzy4F"
      },
      "source": [
        "df_dev_fast = pd.DataFrame(0, index=np.arange(len(df_develop.token)), columns=list(df_fast.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ9egrKmzy4F"
      },
      "source": [
        "for i in tqdm(range(len(df_develop.token))):\n",
        "    if df_develop.temp_toxic[i]==1:\n",
        "        df_dev_fast[df_develop.token[i]][i]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6b27pQxzy4F"
      },
      "source": [
        "**Predict for Naive Bayes Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VReHmZkUzy4F"
      },
      "source": [
        "y_pred=nb.predict(df_dev_fast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FEBxTM5zy4F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPGq7UBDzy4F"
      },
      "source": [
        "doc_id = 0\n",
        "sentences = []\n",
        "temp_doc = []\n",
        "for i in tqdm(range(len(df_develop.doc))):\n",
        "    if df_develop.doc[i]==doc_id :\n",
        "        temp_doc.append(y_pred[i])\n",
        "    else:\n",
        "        sentences.append(temp_doc)\n",
        "        doc_id = df_develop.doc[i]\n",
        "        temp_doc = []\n",
        "        temp_doc.append(y_pred[i])\n",
        "sentences.append(temp_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzA17v92zy4F"
      },
      "source": [
        "df_temp = pd.DataFrame({'pred':sentences[1:]})\n",
        "df_temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuWKUyDPzy4F"
      },
      "source": [
        "df_dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4K7kvbGzy4G"
      },
      "source": [
        "tsd = pd.read_csv(\"data/tsd_trial.csv\") \n",
        "tsd.spans = tsd.spans.apply(literal_eval)\n",
        "tsd.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIKzeEsfzy4G"
      },
      "source": [
        "toxic_offsets_all=[]\n",
        "for i in range(len(df_dev.token)):\n",
        "    toxic_offsets = []\n",
        "    current_offset = 0\n",
        "    for j in range(len(df_dev.token[i])):\n",
        "        if df_temp.pred[i][j]==1:\n",
        "            toxic_offsets.extend(list(range(current_offset, current_offset+len(df_dev.token[i][j]))))\n",
        "        current_offset += len(df_dev.token[i][j]) + 1\n",
        "    toxic_offsets_all.append(toxic_offsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrDWq3jSzy4G"
      },
      "source": [
        "df_predict = pd.DataFrame({'pred':toxic_offsets_all})\n",
        "df_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWZE-Sj3zy4H"
      },
      "source": [
        "tsd[\"predictions\"] = df_predict.pred\n",
        "tsd[\"f1_scores\"] = tsd.apply(lambda row: f1(row.predictions, row.spans), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR4mlzpWzy4H"
      },
      "source": [
        "from scipy.stats import sem\n",
        "print (f\"F1 = {tsd.f1_scores.mean():.2f} ± {sem(tsd.f1_scores):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfZOdZiqzy4H"
      },
      "source": [
        "**Predict for PassiveAggressive Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJhqyu94zy4H"
      },
      "source": [
        "y_pred=pa.predict(df_dev_fast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha0Ml1KIzy4H"
      },
      "source": [
        "doc_id = 0\n",
        "sentences = []\n",
        "temp_doc = []\n",
        "for i in tqdm(range(len(df_develop.doc))):\n",
        "    if df_develop.doc[i]==doc_id :\n",
        "        temp_doc.append(y_pred[i])\n",
        "    else:\n",
        "        sentences.append(temp_doc)\n",
        "        doc_id = df_develop.doc[i]\n",
        "        temp_doc = []\n",
        "        temp_doc.append(y_pred[i])\n",
        "sentences.append(temp_doc)\n",
        "df_temp = pd.DataFrame({'pred':sentences[1:]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB9zSce5zy4I"
      },
      "source": [
        "tsd = pd.read_csv(\"data/tsd_trial.csv\") \n",
        "tsd.spans = tsd.spans.apply(literal_eval)\n",
        "\n",
        "toxic_offsets_all=[]\n",
        "for i in range(len(df_dev.token)):\n",
        "    toxic_offsets = []\n",
        "    current_offset = 0\n",
        "    for j in range(len(df_dev.token[i])):\n",
        "        if df_temp.pred[i][j]==1:\n",
        "            toxic_offsets.extend(list(range(current_offset, current_offset+len(df_dev.token[i][j]))))\n",
        "        current_offset += len(df_dev.token[i][j]) + 1\n",
        "    toxic_offsets_all.append(toxic_offsets)\n",
        "df_predict = pd.DataFrame({'pred':toxic_offsets_all})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBpjF-ezzy4I"
      },
      "source": [
        "tsd[\"predictions\"] = df_predict.pred\n",
        "tsd[\"f1_scores\"] = tsd.apply(lambda row: f1(row.predictions, row.spans), axis=1)\n",
        "print (f\"F1 = {tsd.f1_scores.mean():.2f} ± {sem(tsd.f1_scores):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSXgDVwmzy4I"
      },
      "source": [
        "**Predict for SGD Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfVQl2eIzy4I"
      },
      "source": [
        "y_pred=sgd.predict(df_dev_fast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "945HSJWmzy4I"
      },
      "source": [
        "doc_id = 0\n",
        "sentences = []\n",
        "temp_doc = []\n",
        "for i in tqdm(range(len(df_develop.doc))):\n",
        "    if df_develop.doc[i]==doc_id :\n",
        "        temp_doc.append(y_pred[i])\n",
        "    else:\n",
        "        sentences.append(temp_doc)\n",
        "        doc_id = df_develop.doc[i]\n",
        "        temp_doc = []\n",
        "        temp_doc.append(y_pred[i])\n",
        "sentences.append(temp_doc)\n",
        "df_temp = pd.DataFrame({'pred':sentences[1:]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErLFtI9vzy4I"
      },
      "source": [
        "tsd = pd.read_csv(\"data/tsd_trial.csv\") \n",
        "tsd.spans = tsd.spans.apply(literal_eval)\n",
        "\n",
        "toxic_offsets_all=[]\n",
        "for i in range(len(df_dev.token)):\n",
        "    toxic_offsets = []\n",
        "    current_offset = 0\n",
        "    for j in range(len(df_dev.token[i])):\n",
        "        if df_temp.pred[i][j]==1:\n",
        "            toxic_offsets.extend(list(range(current_offset, current_offset+len(df_dev.token[i][j]))))\n",
        "        current_offset += len(df_dev.token[i][j]) + 1\n",
        "    toxic_offsets_all.append(toxic_offsets)\n",
        "df_predict = pd.DataFrame({'pred':toxic_offsets_all})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwWoWxSWzy4I"
      },
      "source": [
        "tsd[\"predictions\"] = df_predict.pred\n",
        "tsd[\"f1_scores\"] = tsd.apply(lambda row: f1(row.predictions, row.spans), axis=1)\n",
        "print (f\"F1 = {tsd.f1_scores.mean():.2f} ± {sem(tsd.f1_scores):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUwicc9Vzy4J"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGmNTlOezy4J"
      },
      "source": [
        "# System 4 (with spacy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EE9eFZjzy4J"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.metrics.distance import jaccard_distance\n",
        "from nltk.util import ngrams\n",
        "from nltk.metrics.distance  import edit_distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrD7lvZ2zy4J"
      },
      "source": [
        "# Datasets Loading\n",
        "\n",
        "# training data\n",
        "dataset = pd.read_csv(\"data/tsd_train.csv\")\n",
        "# development and validation data\n",
        "tsd = pd.read_csv(\"data/tsd_trial.csv\")\n",
        "\n",
        "dataset.spans = dataset.spans.apply(literal_eval)\n",
        "tsd.spans = tsd.spans.apply(literal_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lps1C_LEzy4J"
      },
      "source": [
        "**Toxic words from lexicons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR1mHsvuzy4J"
      },
      "source": [
        "lex_1 = pd.read_csv(\"lexicon/base_list_bad_words.csv\")\n",
        "\n",
        "lex_2 = pd.read_csv(\"lexicon/swearWords.csv\").T\n",
        "lex_2 = lex_2.reset_index()\n",
        "lex_2 = lex_2.rename(columns={\"index\": \"bad_word\"})\n",
        "\n",
        "lex_3 = pd.read_csv(\"lexicon/Terms-to-Block.csv\")\n",
        "\n",
        "lex_4 = pd.read_table(\"lexicon/bad-words.txt\", delim_whitespace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sEpaQfszy4J"
      },
      "source": [
        "#####################\n",
        "lex_1 = lex_1.apply(lambda x: x.str.replace(';',''))\n",
        "correct_spellings_1 = set(lex_1.bad_word.tolist())\n",
        "\n",
        "lex_1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuK5gorSzy4K"
      },
      "source": [
        "#####################\n",
        "correct_spellings_2 = set(lex_2.bad_word.tolist())\n",
        "lex_2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9XOHChKzy4K"
      },
      "source": [
        "#####################\n",
        "lex_3 = lex_3.apply(lambda x: x.str.replace(',',''))\n",
        "correct_spellings_3 = set(lex_3.bad_word.tolist())\n",
        "lex_3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vl6xg14zy4K"
      },
      "source": [
        "#####################\n",
        "correct_spellings_4 = set(lex_4.bad_word.tolist())\n",
        "lex_4.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRhz9Gnwzy4K"
      },
      "source": [
        "##################### with all lexicons together\n",
        "lexicon_toxic = pd.read_pickle('lexicon_toxic')\n",
        "lexicon_toxic\n",
        "#####################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhXnylUGzy4K"
      },
      "source": [
        "correct_spellings = set(lexicon_toxic.bad_word.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D-2_3c2zy4K"
      },
      "source": [
        "**Start algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QVWwjROzy4L"
      },
      "source": [
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3knuJm3zy4L"
      },
      "source": [
        "# Tranform from a format (character list) to other format (word list)\n",
        "def spans_to_ents(doc, toxic_spans):\n",
        "    started = False\n",
        "    left, right, ents = 0, 0, []\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'SPACE': continue\n",
        "        # if the token is in the spans return just the starting-ending offsets\n",
        "        if toxic_spans.intersection(set(range(token.idx, token.idx + len(token.text)))):\n",
        "            if not started:        \n",
        "                left, started = token.idx, True\n",
        "            right = token.idx + len(token.text)\n",
        "        # this is activated when \"started\" is True and we moved on a non-toxic word,\n",
        "        # so, just return the saved left/right offsets.\n",
        "        elif started:\n",
        "            ents.append((left, right, 'TOXIC'))\n",
        "            started = False\n",
        "    if started: \n",
        "        ents.append((left, right, 'TOXIC'))\n",
        "    return ents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgcBuBEEzy4L"
      },
      "source": [
        "idx=2342\n",
        "print(dataset.iloc[idx].text, dataset.iloc[idx].spans)\n",
        "spans_to_ents(nlp(dataset.iloc[idx].text), set(dataset.iloc[idx].spans))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24y1Js8yzy4L"
      },
      "source": [
        "TRAIN_DATA = []\n",
        "#create the new dataset,which we use as training dataset\n",
        "for _, row in tqdm(dataset.iterrows()):\n",
        "    doc = nlp(row.text)\n",
        "    ents = spans_to_ents(doc, set(row.spans))\n",
        "    TRAIN_DATA.append((doc.text, {'entities': ents}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYqAwmY2zy4g"
      },
      "source": [
        "TRAIN_DATA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-saAFA31zy4g"
      },
      "source": [
        "nlp = spacy.blank('en')\n",
        "\n",
        "# We have name regognition task, so we need\n",
        "# new NER pipe with TOXIC entities\n",
        "ner = nlp.create_pipe(\"ner\")\n",
        "ner.add_label('TOXIC')\n",
        "\n",
        "# add the NER pipe as last component \n",
        "nlp.add_pipe(ner, last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjX0Nf7Vzy4h"
      },
      "source": [
        "# exclude some pipes from being updated during training\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZgAKPl4zy4h"
      },
      "source": [
        "from IPython.display import SVG, display\n",
        "def show_svg():\n",
        "    display(SVG(url='https://spacy.io/training-73950e71e6b59678754a87d6cf1481f9.svg'))\n",
        "    \n",
        "show_svg()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQlAjqiOzy4h"
      },
      "source": [
        "import random \n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "    \n",
        "    #start to train\n",
        "    nlp.begin_training()\n",
        "    \n",
        "    # 30 epochs\n",
        "    for iteration in tqdm(range(34)):\n",
        "        \n",
        "        #Shuffle the train dataset.\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        \n",
        "        losses = {}\n",
        "        \n",
        "        #split in to batches\n",
        "        batches = spacy.util.minibatch(TRAIN_DATA)\n",
        "        \n",
        "        #for each batch, update the annotations\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(texts, annotations, drop=0.5,  losses=losses)\n",
        "            \n",
        "        print(\"Losses\", losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ExmC-J-zy4h"
      },
      "source": [
        "#### Result of develop set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtHW3Hj9zy4h"
      },
      "source": [
        "for _, row in tqdm(tsd.iterrows()):\n",
        "    spacy.displacy.render(nlp(row.text), style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooDLVb3fzy4h"
      },
      "source": [
        "for _, row in tqdm(tsd.iterrows()):\n",
        "    spacy.displacy.render(nlp(row.text), style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfxkPmGzzy4i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhBUNC1zzy4i"
      },
      "source": [
        "toxic_offsets_all=[]\n",
        "for idx in tqdm(range(len(tsd.spans))):\n",
        "    toxic_offsets=[]\n",
        "    for token in nlp(tsd.text[idx]):\n",
        "        if token.ent_type_=='TOXIC':\n",
        "            temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "            toxic_offsets.extend(temp)\n",
        "            \n",
        "            \n",
        "        else:\n",
        "            token_str= str(token.text).lower()\n",
        "            if token_str in correct_spellings_1:\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "                print('lex_1---->'+str(token_str))\n",
        "            elif token_str in correct_spellings_2:\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "                print('lex_2---->'+str(token_str))\n",
        "            elif token_str in correct_spellings_3:\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "                print('lex_3---->'+str(token_str))\n",
        "            elif token_str=='g&m':\n",
        "                trmp = token_str\n",
        "                # tranform money to TAG (eg 34$ )\n",
        "                token_str= re.sub('g&m', 'gm', token_str)\n",
        "            elif token_str=='fukc':\n",
        "                print(token_str)\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "            elif token_str=='fukcer':\n",
        "                print(token_str)\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "            elif token_str=='sh__' or token_str=='sh-t' or token_str=='muslims':\n",
        "                print(token_str)\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "            else:\n",
        "                trmp = token_str\n",
        "                # tranform percentage to TAG (eg 34% )\n",
        "                token_str= re.sub(r'\\d+[%]', 'NUM', token_str)\n",
        "    \n",
        "                # tranform money to TAG (eg 34$ )\n",
        "                token_str= re.sub(r'\\d+[$]', 'MONEY', token_str)\n",
        "    \n",
        "                # tranform money to TAG (eg $34)\n",
        "                token_str= re.sub(r'[$]\\d+', 'MONEY', token_str)\n",
        "    \n",
        "                # tranform money to TAG (eg $34)\n",
        "                token_str= re.sub(r'[$]\\w+', 'TOXIC', token_str)\n",
        "            \n",
        "                # change hide toxic words (e.g f@@k = f**k)\n",
        "                token_str= re.sub(r\"(\\w+[*+$#@&%!]+\\w+)\", 'TOXIC', token_str)\n",
        "    \n",
        "                # change hide toxic words (e.g f***)\n",
        "                token_str= re.sub(r\"(\\w+[*$#@&%])\", 'TOXIC', token_str)\n",
        "                \n",
        "                if token_str=='TOXIC':\n",
        "                    print(trmp)\n",
        "                    temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                    toxic_offsets.extend(temp)\n",
        "    toxic_offsets_all.append(toxic_offsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dhS8xNzy4i"
      },
      "source": [
        "**Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsUB7A1dzy4i"
      },
      "source": [
        "df_predict = pd.DataFrame({'pred':toxic_offsets_all})\n",
        "df_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iIaxJHfzy4i"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set) + len(gold_set)\n",
        "    return float(nom)/float(denom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHObW1P5zy4j"
      },
      "source": [
        "tsd[\"predictions\"] = df_predict.pred\n",
        "tsd[\"f1_scores\"] = tsd.apply(lambda row: f1(row.predictions, row.spans), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02OGW2QKzy4j"
      },
      "source": [
        "from scipy.stats import sem\n",
        "print (f\"F1 = {tsd.f1_scores.mean():.2f} ± {sem(tsd.f1_scores):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Obz-XdFzy4j"
      },
      "source": [
        "#### Result of test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJLn-m0yzy4j"
      },
      "source": [
        "# test data\n",
        "dataset_test = pd.read_csv(\"data/tsd_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tcLm69Qzy4j"
      },
      "source": [
        "for _, row in tqdm(dataset_test.iterrows()):\n",
        "    spacy.displacy.render(nlp(row.text), style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDiWQGjFzy4j"
      },
      "source": [
        "toxic_offsets_all=[]\n",
        "for idx in tqdm(range(len(dataset_test.text))):\n",
        "    toxic_offsets=[]\n",
        "    for token in nlp(dataset_test.text[idx]):\n",
        "        if token.ent_type_=='TOXIC':\n",
        "            temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "            toxic_offsets.extend(temp)\n",
        "            \n",
        "        else:\n",
        "            token_str= str(token.text).lower()\n",
        "            if token_str in correct_spellings_1:\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "                print('lex_1---->'+str(token_str))\n",
        "            elif token_str in correct_spellings_2:\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "                print('lex_2---->'+str(token_str))\n",
        "            elif token_str in correct_spellings_3:\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "                print('lex_3---->'+str(token_str))\n",
        "            elif token_str=='g&m':\n",
        "                trmp = token_str\n",
        "                # tranform money to TAG (eg 34$ )\n",
        "                token_str= re.sub('g&m', 'gm', token_str)\n",
        "            elif token_str=='fukc':\n",
        "                print(token_str)\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "            elif token_str=='fukcer':\n",
        "                print(token_str)\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "            elif token_str=='sh__' or token_str=='sh-t' or token_str=='muslims':\n",
        "                print(token_str)\n",
        "                temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                toxic_offsets.extend(temp)\n",
        "            else:\n",
        "                trmp = token_str\n",
        "                # tranform percentage to TAG (eg 34% )\n",
        "                token_str= re.sub(r'\\d+[%]', 'NUM', token_str)\n",
        "    \n",
        "                # tranform money to TAG (eg 34$ )\n",
        "                token_str= re.sub(r'\\d+[$]', 'MONEY', token_str)\n",
        "    \n",
        "                # tranform money to TAG (eg $34)\n",
        "                token_str= re.sub(r'[$]\\d+', 'MONEY', token_str)\n",
        "    \n",
        "                # tranform money to TAG (eg $34)\n",
        "                token_str= re.sub(r'[$]\\w+', 'TOXIC', token_str)\n",
        "            \n",
        "                # change hide toxic words (e.g f@@k = f**k)\n",
        "                token_str= re.sub(r\"(\\w+[*+$#@&%!]+\\w+)\", 'TOXIC', token_str)\n",
        "    \n",
        "                # change hide toxic words (e.g f***)\n",
        "                token_str= re.sub(r\"(\\w+[*$#@&%])\", 'TOXIC', token_str)\n",
        "                \n",
        "                if token_str=='TOXIC':\n",
        "                    print(trmp)\n",
        "                    temp = np.arange(token.idx, token.idx + len(token)).tolist()\n",
        "                    toxic_offsets.extend(temp)\n",
        "    toxic_offsets_all.append(toxic_offsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Fkhc_3zy4k"
      },
      "source": [
        "df_predict = pd.DataFrame({'pred':toxic_offsets_all})\n",
        "df_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udVK6JH6zy4l"
      },
      "source": [
        "predictions = df_predict.pred.to_list()\n",
        "ids = df_predict.index.to_list()\n",
        "\n",
        "# write in a prediction file named \"spans-pred.txt\"\n",
        "with open(\"spans-pred.txt\", \"w\") as out:\n",
        "      for uid, text_scores in zip(ids, predictions):\n",
        "            out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_oa7s3ozy4l"
      },
      "source": [
        "! zip -r random_predictions_spacy.zip ./spans-pred.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuG0pl6dzy4l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}